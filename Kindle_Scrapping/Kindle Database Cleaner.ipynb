{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on an eye test, we observed that the original Amazon Kindle Metadata file we were given had many rows with missing data (title, price, description, . This jupyter notebook details the process of how we restored the relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve list of asins from original Amazon Kindle Metadata\n",
    "asin_meta = []\n",
    "f=open('meta_Kindle_Store.json') # Replace line with location of original Amazon Kindle Metadata\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    asin_meta.append(line[10:20])\n",
    "\n",
    "#print(asin_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'try3.csv' is scrapped amazon book data we found from kaggle and cleaned to use for processing\n",
    "# Kaggle URL -> https://www.kaggle.com/ucffool/amazon-sales-rank-data-for-print-and-kindle-books?select=amazon_com_extras.csv\n",
    "df = pd.read_csv('try3.csv')\n",
    "df.head()\n",
    "#print(len(df))\n",
    "\n",
    "price_ls = np.zeros(len(df)) # Append price = 0 to all rows as price parameter is not available for this dataset\n",
    "#print(type(price_ls))\n",
    "\n",
    "description_ls = []\n",
    "for i in range(len(df)):\n",
    "    description_ls.append('nil') # Append description = 'nil' to all rows as description parameter is not available for this dataset\n",
    "#print(description_ls)\n",
    "\n",
    "# Attach price and description columns to dataframe\n",
    "df['PRICE'] = price_ls\n",
    "df['DESCRIPTION'] = description_ls\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(['GROUP', 'FORMAT', 'PUBLISHER'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df into a dictionary to be used for processing later on\n",
    "title_author_dic = df.set_index('ASIN').T.to_dict('list')\n",
    "#title_author_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to attach relevant information from df to original metadata asin\n",
    "start_time = time.time()\n",
    "resultDict = {}\n",
    "for elem in asin_meta:\n",
    "    if elem in title_author_dic:\n",
    "        resultDict[elem] = title_author_dic[elem]\n",
    "    else:\n",
    "        resultDict[elem] = False\n",
    "\n",
    "print(time.time()-start_time)\n",
    "print(resultDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many rows of information was added to the metadata asin \n",
    "count = 0\n",
    "for i in resultDict.values():\n",
    "    if i != False:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Kindle_Book_Dataset2.csv' is another scrapped amazon book data we found from kaggle and cleaned to use for processing\n",
    "# Kaggle URL -> https://www.kaggle.com/snathjr/kindle-books-dataset\n",
    "df_2 = pd.read_csv('Kindle_Book_Dataset2.csv')\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this dataset, we did not have asins as a parameter, we had to pre-process and split the asin from the back of the url parameter\n",
    "url_ls = df_2['url'].tolist()\n",
    "url_ls\n",
    "\n",
    "# Algorithm for splitting url and asin \n",
    "asin_ls = []\n",
    "for i in url_ls:\n",
    "    asin_ls.append(i.rsplit('/', 1)[1])\n",
    "asin_ls\n",
    "\n",
    "# Attach asin back to dataframe\n",
    "df_2['asin'] = asin_ls\n",
    "#df_2.head()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_2.drop(['url', 'save', 'pages', 'size', 'publisher', 'language', 'text_to_speech', 'x_ray', 'lending', 'customer_reviews', 'stars'], axis=1, inplace=True)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df into a dictionary to be used for processing later on\n",
    "title_author_dic_2 = df_2.set_index('asin').T.to_dict('list')\n",
    "#title_author_dic_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to attach relevant information from df to original metadata asin\n",
    "start_time = time.time()\n",
    "\n",
    "for elem in resultDict:\n",
    "    if resultDict[elem] == False and elem in title_author_dic_2:\n",
    "        resultDict[elem] = title_author_dic_2[elem]\n",
    "\n",
    "print(time.time()-start_time)\n",
    "print(resultDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many rows of information was added to the metadata asin \n",
    "count = 0\n",
    "for i in resultDict.values():\n",
    "    if i != False:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "for i in resultDict:\n",
    "    if resultDict[i] != False:\n",
    "        print(resultDict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store apended data to file\n",
    "with open('scrapped_data_with_desc.json','w') as outfile:\n",
    "    json.dump(resultDict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to seperate the rows with full information into a consolidated file\n",
    "start_time = time.time()\n",
    "resultDict_2 = {}\n",
    "for elem in asin_meta:\n",
    "    if elem in title_author_dic_2:\n",
    "        resultDict_2[elem] = title_author_dic_2[elem]\n",
    "    else:\n",
    "        resultDict_2[elem] = False\n",
    "\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many rows of information have full parameters\n",
    "count = 0\n",
    "for i in resultDict_2.values():\n",
    "    if i != False:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save appended results to file\n",
    "with open('mini_scrapped_12k.json','w') as outfile:\n",
    "    json.dump(resultDict_2, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is how we attach the retrieved data to the original metadata file based on the asin parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file with scrapped data and asins\n",
    "f2=open('scrapped_data_with_desc.json')\n",
    "for line in f2:\n",
    "    x = line\n",
    "    #obj = eval(line)\n",
    "    #print(type(obj))\n",
    "\n",
    "res = json.loads(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to attach back asin with found data back to original metadata\n",
    "# 'meta_Kindle_Store_Compiled_3.json' is the new file we want to write to\n",
    "compiled_file = open('meta_Kindle_Store_Compiled_3.json', 'w')\n",
    "\n",
    "working_data=open('meta_Kindle_Store.json', 'r')\n",
    "for line in working_data:\n",
    "    #print(line)\n",
    "    #print(type(line))\n",
    "    line_dic = eval(line)\n",
    "    asin = line_dic['asin']\n",
    "    if asin in res:\n",
    "        if res[asin] != False:\n",
    "            if 'title' not in line_dic.keys():\n",
    "                line_dic['title'] = res[asin][0]\n",
    "            if 'price' not in line_dic.keys():\n",
    "                line_dic['price'] = res[asin][2]\n",
    "            if 'description' not in line_dic.keys():\n",
    "                line_dic['description'] = res[asin][3]\n",
    "            line_dic['author'] = res[asin][1]\n",
    "        else:\n",
    "            if 'title' not in line_dic.keys():\n",
    "                line_dic['title'] = 'nil'\n",
    "            if 'price' not in line_dic.keys():\n",
    "                line_dic['price'] = 0.0\n",
    "            if 'description' not in line_dic.keys():\n",
    "                line_dic['description'] = 'nil'\n",
    "            line_dic['author'] = 'nil'\n",
    "    compiled_file.writelines(json.dumps(line_dic) + '\\n') \n",
    "\n",
    "working_data.close()\n",
    "compiled_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up file with only full asin data\n",
    "f3=open('mini_scrapped_12k.json')\n",
    "for line in f3:\n",
    "    x_2 = line\n",
    "res_2 = json.loads(x_2)\n",
    "print(type(res_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate json file with only full rows of information\n",
    "compiled_file_2 = open('meta_Kindle_Store_mini.json', 'w')\n",
    "\n",
    "working_data=open('meta_Kindle_Store.json', 'r')\n",
    "for line in working_data:\n",
    "    #print(line)\n",
    "    #print(type(line))\n",
    "    line_dic = eval(line)\n",
    "    asin = line_dic['asin']\n",
    "    if asin in res_2:\n",
    "        if res_2[asin] != False:\n",
    "            if 'title' not in line_dic.keys():\n",
    "                line_dic['title'] = res_2[asin][0]\n",
    "            if 'price' not in line_dic.keys():\n",
    "                line_dic['price'] = res_2[asin][2]\n",
    "            if 'description' not in line_dic.keys():\n",
    "                line_dic['description'] = res_2[asin][3]\n",
    "            line_dic['author'] = res_2[asin][1]\n",
    "            compiled_file_2.writelines(json.dumps(line_dic) + '\\n') \n",
    "        else:\n",
    "            continue\n",
    "            #if 'title' not in line_dic.keys():\n",
    "                #line_dic['title'] = 'nil'\n",
    "            #if 'price' not in line_dic.keys():\n",
    "                #line_dic['price'] = 0.0\n",
    "            #if 'description' not in line_dic.keys():\n",
    "                #line_dic['description'] = 'nil'\n",
    "            #line_dic['author'] = 'nil'\n",
    "\n",
    "working_data.close()\n",
    "compiled_file_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm to upload data into Mongodb server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymongo\n",
    "#from pymongo import MongoClient\n",
    "\n",
    "url = \"mongodb+srv://jeroee:jerokok97@testdb.cpfwr.mongodb.net/test?authSource=admin&replicaSet=atlas-13ih9s-shard-0&readPreference=primary&appname=MongoDB%20Compass&ssl=true\"\n",
    "client = pymongo.MongoClient(url)  #connecting to mongo atlas\n",
    "db = client.get_database('testDb') #connecting to database called testDb\n",
    "meta_Kindle_12k = db.meta_Kindle_12k  \n",
    "\n",
    "\n",
    "file = open(\"meta_Kindle_Store_mini.json\")\n",
    "count = 0\n",
    "for line in file:\n",
    "    try:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        obj = eval(line)\n",
    "        meta_Kindle_12k.insert_one(obj)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"number of object added\", count\n",
    ")\n",
    " \n",
    "#just to check if correct number of entries added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
